{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "\n",
    "!pip install torch -U\n",
    "!pip install transformers -U\n",
    "!pip install -i https://pypi.org/simple/ -U bitsandbytes\n",
    "!pip install accelerate -U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"abacusai/Llama-3-Smaug-8B\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quanty_type = \"fp4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quanty = True,\n",
    ")\n",
    "\n",
    "amodel = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config = bnb_config,\n",
    "    torch_dtype = torch.float16,\n",
    "    device_map = \"auto\",\n",
    "    trust_remote_code = True,\n",
    ")\n",
    "atokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2-7B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quanty_type = \"fp4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quanty = True,\n",
    ")\n",
    "\n",
    "bmodel = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config = bnb_config,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    device_map = \"auto\",\n",
    "    trust_remote_code = True,\n",
    ")\n",
    "btokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = pd.read_csv('datasets/finalkeywords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "questions = []\n",
    "answers = []\n",
    "guesses = []\n",
    "turns = 0\n",
    "thingCategory = False\n",
    "guessWithModel = True\n",
    "guesses_set = set()  # Set to track unique guesses\n",
    "import string\n",
    "\n",
    "def generate_text(prompt, sys_prompt=\"\", max_new_tokens=700, top_p=0.9, top_k=50, isGuesser=False):\n",
    "    global amodel, atokenizer, bmodel, btokenizer\n",
    "    if isGuesser:\n",
    "        model = amodel\n",
    "        tokenizer = atokenizer\n",
    "    else:\n",
    "        model = bmodel\n",
    "        tokenizer = btokenizer\n",
    "\n",
    "    \"\"\"Generate text using the model based on the given prompt and system prompt.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "    pad_token_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        attention_mask=model_inputs.attention_mask,\n",
    "        pad_token_id=pad_token_id,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k\n",
    "    )\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    response = response.strip()\n",
    "\n",
    "    # Clean up response\n",
    "    if \"assistant\" in response:\n",
    "        response = response.replace(\"assistant\", \"\").strip()\n",
    "    if \"system\" in response:\n",
    "        response = response.replace(\"system\", \"\").strip()\n",
    "\n",
    "    # Post-process the response to remove extraneous text\n",
    "    response_lines = response.split('\\n')\n",
    "    if len(response_lines) > 1:\n",
    "        response = response_lines[-1].strip()\n",
    "\n",
    "    return response\n",
    "\n",
    "def context_formatter(questions, answers, guesses):\n",
    "    global amodel, atokenizer\n",
    "    \"\"\"Format the context as a string with each question-answer pair and wrong guesses, then truncate it to fit within the model's maximum token length.\"\"\"\n",
    "\n",
    "    # Select the appropriate model and tokenizer\n",
    "    model = amodel\n",
    "    tokenizer = atokenizer\n",
    "\n",
    "    # Retrieve the maximum position embeddings from the selected model\n",
    "    max_length = model.config.max_position_embeddings\n",
    "\n",
    "    context_pairs = [f\"Q: {q} A: {a}\" for q, a in zip(questions, answers)]\n",
    "    context_pairs.append(f\"Guessed keywords: {', '.join(guesses)} are wrong\")\n",
    "    formatted_context = \". \".join(context_pairs)\n",
    "\n",
    "    context_tokens = tokenizer.encode(formatted_context, return_tensors='pt').to('cuda')\n",
    "    if context_tokens.shape[1] > max_length:\n",
    "        truncated_context = tokenizer.decode(context_tokens[0, -max_length:], skip_special_tokens=True)\n",
    "    else:\n",
    "        truncated_context = formatted_context\n",
    "\n",
    "    return truncated_context\n",
    "\n",
    "def generate_question_with_llm(context):\n",
    "    global turns\n",
    "    if turns == 0:\n",
    "        return \"Is it a city or country and not a thing or object? If it is an object, thing, or animal say no.\"\n",
    "    elif turns == 1:\n",
    "        if len(answers) > 0 and answers[0].lower() == \"yes\":\n",
    "            return \"Is it a city?\"\n",
    "        else:\n",
    "            return \"Is it a living thing?\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are playing a game of 20 Questions. Generate a new question to help narrow down the possibilities.\n",
    "    Previous Q&A:\n",
    "    {context}\n",
    "    Next question:\n",
    "    \"\"\"\n",
    "    sys_prompt = \"Generate a clear and specific yes or no question based on the context provided. Provide only the question.\"\n",
    "\n",
    "    question = \"\"\n",
    "    for _ in range(5):\n",
    "        question = generate_text(prompt, sys_prompt, max_new_tokens=900, isGuesser=True)\n",
    "        if question and question not in questions:\n",
    "            return question\n",
    "\n",
    "    return question\n",
    "\n",
    "def guess_based_on_qa(context):\n",
    "    global guesses_set  # Use a set to keep track of guesses\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Based on the following questions and answers, guess the keyword:\n",
    "    {context}\n",
    "    Your guess:\n",
    "    \"\"\"\n",
    "    sys_prompt = \"\"\"\n",
    "    Review the context and give your best guess in one or two words.\n",
    "    Only provide the keyword, without any additional text or formatting.\"\"\"\n",
    "\n",
    "    guess = \"\"\n",
    "    max_attempts = 5  # Limit the number of attempts to find a unique guess\n",
    "\n",
    "    for _ in range(max_attempts):\n",
    "        guess = generate_text(prompt, sys_prompt, max_new_tokens=5, isGuesser=True)  # Adjust max_new_tokens for the guesser\n",
    "        guess = guess.translate(str.maketrans('', '', string.punctuation)).strip()\n",
    "\n",
    "        if guess and guess not in guesses_set:\n",
    "            guesses_set.add(guess)\n",
    "            return guess\n",
    "\n",
    "    # If all attempts failed, return an empty string or a default value\n",
    "    return guess\n",
    "\n",
    "def guess_with_dataset(context):\n",
    "    global keywords, thingCategory, guesses_set  # Use a set to keep track of guesses\n",
    "\n",
    "    # Filter out guessed keywords\n",
    "    remaining_keywords = keywords[~keywords['Keyword'].isin(guesses_set)]\n",
    "\n",
    "    if thingCategory:\n",
    "        keys = ','.join(remaining_keywords[remaining_keywords['Category'] == 'things']['Keyword'])\n",
    "    else:\n",
    "        keys = ','.join(remaining_keywords[remaining_keywords['Category'] == 'place']['Keyword'])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Based on the following questions and answers, guess the keyword:\n",
    "    {context}\n",
    "    You can choose from the following keywords: {keys}\n",
    "    Your guess:\n",
    "    \"\"\"\n",
    "\n",
    "    sys_prompt = \"\"\"\n",
    "    Review the context and the list of potential keywords. Give your best guess from the list in one or two words.\n",
    "    Only provide the keyword, without any additional text or formatting.\"\"\"\n",
    "\n",
    "    guess = \"\"\n",
    "    max_attempts = 5  # Limit the number of attempts to find a unique guess\n",
    "\n",
    "    for _ in range(max_attempts):\n",
    "        guess = generate_text(prompt, sys_prompt, max_new_tokens=3, isGuesser=True)  # Adjust max_new_tokens for the guesser\n",
    "        guess = guess.translate(str.maketrans('', '', string.punctuation)).strip()\n",
    "\n",
    "        if guess not in guesses_set:\n",
    "            print(guess)\n",
    "            print(guesses_set)\n",
    "            guesses_set.add(guess)\n",
    "            if guess in keys:\n",
    "                keywords = keywords.drop(keywords[keywords['Keyword'] == guess].index)\n",
    "            return guess\n",
    "\n",
    "    # If all attempts failed, return an empty string or a default value\n",
    "    return guess\n",
    "\n",
    "def get_yes_no(question, keyword):\n",
    "    \"\"\"Get a yes or no answer to the given question based on the specified keyword.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are playing 20 Questions. Answer 'yes' or 'no' for this keyword: {keyword}\n",
    "\n",
    "    Question: \"{question}\"\n",
    "\n",
    "    Rules:\n",
    "    1. Consider only your keyword: {keyword}.\n",
    "    2. Answer strictly 'yes' or 'no'. Nothing else.\n",
    "    3. Do not provide any explanations or additional information.\n",
    "    Your answer:\n",
    "    \"\"\"\n",
    "    sys_prompt = \"Answer strictly 'yes' or 'no' to the following question and nothing else.\"\n",
    "\n",
    "    response = generate_text(prompt, sys_prompt, max_new_tokens=1, isGuesser=False)  # Reduced token limit for strict answers\n",
    "\n",
    "    # Post-process response to ensure it's only \"yes\" or \"no\"\n",
    "    response = response.strip().lower()\n",
    "    if \"yes\" in response:\n",
    "        return \"yes\"\n",
    "    elif \"no\" in response:\n",
    "        return \"no\"\n",
    "    else:\n",
    "        # In case of unexpected response, default to \"no\" to ensure safety\n",
    "        return \"no\"\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    global turns, amodel, questions, answers, guesses, guesses_set, VERBOSE, guessWithModel, thingCategory\n",
    "\n",
    "    if obs.turnType == \"ask\":\n",
    "        context = context_formatter(questions, answers, guesses)\n",
    "        response = generate_question_with_llm(context)\n",
    "        questions.append(response)\n",
    "        turns += 1\n",
    "    elif obs.turnType == \"guess\":\n",
    "        context = context_formatter(questions, answers, guesses)\n",
    "\n",
    "        if guessWithModel:\n",
    "            if len(answers) == 1 and answers[0].lower() == 'no':\n",
    "                thingCategory = True\n",
    "            response = guess_with_dataset(context)\n",
    "        else:\n",
    "            response = guess_based_on_qa(context)\n",
    "        if response:  # Only add non-empty guesses\n",
    "            guesses.append(response)\n",
    "    else:  # obs.turnType == \"answer\"\n",
    "\n",
    "        response = get_yes_no(obs.questions[-1], obs.keyword)\n",
    "        answers.append(response)\n",
    "\n",
    "    # Display role\n",
    "    if VERBOSE:\n",
    "        if obs.turnType == \"answer\":\n",
    "            print(f\"Team 2 - Answerer - ### Agent LLAMA 8B ###\")\n",
    "        else:\n",
    "            print(f\"\\nTeam 2 - Questioner - ### Agent LLAMA 8B ###\")\n",
    "        print(f\"OUTPUT = '{response}'\")\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "perform_eval = True\n",
    "if perform_eval:\n",
    "    torch.cuda.empty_cache()\n",
    "    import requests\n",
    "    import json\n",
    "    import re\n",
    "    import typing as t\n",
    "    import random\n",
    "    import time\n",
    "    from IPython.display import display, Markdown\n",
    "    import signal\n",
    "\n",
    "    # ===============FETCH LATEST KEYWORDS FROM KAGGLE GITHUB================\n",
    "    # This does not train the model to look for the keywords.\n",
    "    # This just fetches the list so it can be used in the evaluation.\n",
    "\n",
    "    url = \"https://raw.githubusercontent.com/Kaggle/kaggle-environments/master/kaggle_environments/envs/llm_20_questions/keywords.py\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        match = re.search(r'KEYWORDS_JSON = \"\"\"(.*?)\"\"\"', response.text, re.DOTALL)\n",
    "        if match:\n",
    "            json_str = match.group(1)\n",
    "            keywords_dict = json.loads(json_str)\n",
    "        else:\n",
    "            print(\"Could not find the KEYWORDS_JSON variable in the file.\")\n",
    "    else:\n",
    "        print(\"Request failed with status code:\", response.status_code)\n",
    "        import time\n",
    "\n",
    "    def select_random_keyword(keywords_dict: t.List[dict]) -> str:\n",
    "        category = random.choice(keywords_dict)\n",
    "        keyword_dict = random.choice(category['words'])\n",
    "        return keyword_dict['keyword'].lower()\n",
    "\n",
    "    #===============20 QUESTIONS EVALUATION SESSION=====================\n",
    "    def input_timeout(prompt, timeout):\n",
    "        def timeout_handler(signum, frame):\n",
    "            raise TimeoutError(\"Input timed out after {} seconds\".format(timeout))\n",
    "\n",
    "        signal.signal(signal.SIGALRM, timeout_handler)\n",
    "        signal.alarm(timeout)\n",
    "\n",
    "        try:\n",
    "            user_input = input(prompt)\n",
    "            signal.alarm(0)\n",
    "            return user_input\n",
    "        except TimeoutError as e:\n",
    "            print(e)\n",
    "            return None\n",
    "\n",
    "    timeout = 10\n",
    "\n",
    "    try:\n",
    "        DEBUG_input = input_timeout(\"Enable verbose debug mode? (y/n) [default is n] \", timeout)\n",
    "        DEBUG = DEBUG_input.lower() == 'y' if DEBUG_input else False\n",
    "    except:\n",
    "        DEBUG = False\n",
    "        print(\"No input received, defaulting to false.\")\n",
    "\n",
    "    class MockObservation:\n",
    "        def __init__(self, step: int, role: str, turnType: str, keyword: str, category: str, questions: list[str], answers: list[str], guesses: list[str]):\n",
    "            self.step = step\n",
    "            self.role = role\n",
    "            self.turnType = turnType\n",
    "            self.keyword = keyword\n",
    "            self.category = category\n",
    "            self.questions = questions\n",
    "            self.answers = answers\n",
    "            self.guesses = guesses\n",
    "\n",
    "    def test_20_questions():\n",
    "        global DEBUG\n",
    "        step = 0\n",
    "        role = \"answerer\"\n",
    "        turnType = \"ask\"\n",
    "        keyword = select_random_keyword(keywords_dict)\n",
    "        category = \"\"\n",
    "        questions = []\n",
    "        answers = []\n",
    "        guesses = []\n",
    "        display(Markdown(\"# Starting 20 questions eval game...\"))\n",
    "        display(Markdown(f\"### **Keyword:** {keyword}\"))\n",
    "\n",
    "        for i in range(60):\n",
    "            obs = MockObservation(step, role, turnType, keyword, category, questions, answers, guesses)\n",
    "\n",
    "            start_time = time.time()\n",
    "            response = agent_fn(obs, None)\n",
    "            end_time = time.time()\n",
    "\n",
    "            response_time = end_time - start_time\n",
    "            if response_time > 60:\n",
    "                display(Markdown(f\"**WARNING:** Response time too long and may be disqualified from the game: {response_time:.2f} sec. Make sure you have GPU acceleration enabled in the session options on the right side panel.\"))\n",
    "                break\n",
    "\n",
    "            # Record the response in the appropriate list\n",
    "            if turnType == 'ask':\n",
    "                questions.append(response)\n",
    "                turnType = 'answer'\n",
    "            elif turnType == 'answer':\n",
    "                answers.append(response)\n",
    "                turnType = 'guess'\n",
    "            elif turnType == 'guess':\n",
    "                guesses.append(response)\n",
    "                if response.lower() == keyword.lower():\n",
    "                    display(Markdown(f\"## **Keyword '{keyword}' guessed correctly! Ending game.**\"))\n",
    "                    break\n",
    "                turnType = 'ask'\n",
    "                step += 1\n",
    "\n",
    "            display(Markdown(f\"Step {step} | Response: {response} | {response_time:.2f} sec\"))\n",
    "        display(Markdown(f\"Final Questions: {', '.join(questions)}\"))\n",
    "        display(Markdown(f\"Final Answers: {', '.join(answers)}\"))\n",
    "        display(Markdown(f\"Final Guesses: {', '.join(guesses)}\"))\n",
    "\n",
    "    # Run the test\n",
    "    test_20_questions()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
